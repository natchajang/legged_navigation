{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/natcha/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/natcha/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.2.1+cu121\n",
      "Device count 1\n",
      "/home/natcha/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/natcha/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/natcha/.cache/torch_extensions/py38_cu121/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    }
   ],
   "source": [
    "from isaacgym.torch_utils import *\n",
    "from isaacgym import gymtorch, gymapi, gymutil\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation env frame to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base position and orientation express on env frame\n",
    "x = 0.0\n",
    "y = 3.0\n",
    "z = 2.0\n",
    "roll = torch.tensor([0])\n",
    "pitch = torch.tensor([math.pi/2])\n",
    "yaw = torch.tensor([math.pi/2])\n",
    "\n",
    "orientation = quat_from_euler_xyz(roll, pitch, yaw)\n",
    "rx = orientation[0][0]\n",
    "ry = orientation[0][1]\n",
    "rz = orientation[0][2]\n",
    "rw = orientation[0][3]\n",
    "\n",
    "# goal pos on env frame\n",
    "g_x = 1.0\n",
    "g_y = 6.0\n",
    "g_z = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vec3(-1.000000, -1.000000, 3.000000)\n",
      "Vec3(1.000000, 5.999999, 3.000000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = gymapi.Transform(p = gymapi.Vec3(x, y, z), r=gymapi.Quat(rx, ry, rz, rw))\n",
    "\n",
    "transform_inv = transform.inverse()\n",
    "pos_on_base = transform_inv.transform_point(gymapi.Vec3(g_x, g_y, g_z))\n",
    "print(pos_on_base)\n",
    "print(transform.transform_point(pos_on_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply by my self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0.,  0.],\n",
      "        [-0.,  1., -0., -3.],\n",
      "        [ 0.,  0.,  1., -2.],\n",
      "        [ 0.,  0.,  0.,  1.]])\n",
      "tensor([[1., 6., 3., 1.]])\n",
      "tensor([[0., 3., 2.]])\n",
      "tensor([[-1.0000,  3.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "base_quat = orientation\n",
    "root_states = torch.tensor([[x, y, z]])\n",
    "goal_states = torch.tensor([[g_x, g_y, g_z, 1]])\n",
    "T = torch.tensor([[1, 0, 0, x], [0, 1, 0, y], [0, 0, 1, z], [0, 0, 0, 1]])\n",
    "T_inv = torch.inverse(T)\n",
    "\n",
    "base = (torch.matmul(T_inv, goal_states.T).T)[:, : -1]\n",
    "base = quat_rotate_inverse(base_quat, base)\n",
    "\n",
    "print(T_inv)\n",
    "print(goal_states)\n",
    "print(root_states)\n",
    "print(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_inverse(q:torch.tensor, t:torch.tensor, g:torch.tensor):\n",
    "    \"\"\"\n",
    "    q is orientation of frame2 (agent) express on frame1 (env)\n",
    "    t is origin position of frame2 (agent) relative to frame1 (env)\n",
    "    g is point position on frame1 (env)\n",
    "    \"\"\"\n",
    "    t_extend = torch.cat([t, torch.ones(t.shape[0], 1)], dim=-1)\n",
    "    T_inv = quat_conjugate(t_extend)[:, :-1]\n",
    "    a = T_inv + g                          # inverse translation\n",
    "    b = quat_rotate_inverse(q, a)          # inverse rotation\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000,  3.0000,  1.0000],\n",
       "        [-1.0000,  3.0000,  1.0000],\n",
       "        [-1.0000,  3.0000,  1.0000],\n",
       "        [-1.0000,  3.0000,  1.0000],\n",
       "        [-1.0000,  3.0000,  1.0000]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_env = 5\n",
    "root_states = torch.tensor([x, y, z])\n",
    "goal_pos = torch.tensor([g_x, g_y, g_z])\n",
    "\n",
    "root_states = root_states.repeat((num_env, 1))\n",
    "base_quat_real = base_quat.repeat((num_env,1))\n",
    "goal_pos = goal_pos.repeat((num_env, 1))\n",
    "\n",
    "transformation_inverse(base_quat_real, root_states, goal_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-20.,  25.])\n",
      "tensor([-1.0000,  0.9806])\n"
     ]
    }
   ],
   "source": [
    "base_vel_xy = torch.tensor([[-2.0, -2.0], [2.0, 3.0]])\n",
    "diff_distance = torch.tensor([[5.0, 5.0], [5.0, 5.0]]) - torch.tensor([[0.0, 0.0], [0.0, 0.0]])\n",
    "\n",
    "norm_vel = torch.norm(base_vel_xy, dim=1)\n",
    "norm_distance = torch.norm(diff_distance, dim=1)\n",
    "\n",
    "dot_product = torch.sum(base_vel_xy * diff_distance, dim=1)\n",
    "\n",
    "print(dot_product)\n",
    "print(dot_product / (norm_vel * norm_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([[-2.0, -2.0, 1.0], [2.0, 3.0, 2.0]])\n",
    "print(torch.clip(test, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
